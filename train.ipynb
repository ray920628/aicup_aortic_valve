{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ! pip uninstall -y torch torchvision torchaudio\n",
        "# # 裝含 CUDA 12.4 的官方輪子\n",
        "# ! pip install --upgrade torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu124"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ! pip install -U pip\n",
        "# ! pip uninstall -y ultralytics  # 若已裝舊版，建議先移除\n",
        "# ! pip install git+https://github.com/sunsmarterjie/yolov12.git\n",
        "# ! pip install -U pip setuptools wheel\n",
        "# ! pip install huggingface_hub transformers safetensors accelerate\n",
        "# ! pip install scikit-learn\n",
        "# ! pip install pandas"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "##  前處理"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WvSB8Sr3hWGy",
        "outputId": "4c80a127-540a-407b-9fd3-6e2c2a634de1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "IMG_ROOT = C:\\Users\\307\\Desktop\\aicup\\training_image_roi\n",
            "LBL_ROOT = C:\\Users\\307\\Desktop\\aicup\\training_label_roi\n",
            "[OK] 建立連結：C:\\Users\\307\\Desktop\\aicup\\dataset\\images → C:\\Users\\307\\Desktop\\aicup\\training_image_roi\n",
            "[OK] 建立連結：C:\\Users\\307\\Desktop\\aicup\\dataset\\labels → C:\\Users\\307\\Desktop\\aicup\\training_label_roi\n"
          ]
        }
      ],
      "source": [
        "import os, glob, subprocess, sys\n",
        "from pathlib import Path\n",
        "from ultralytics import YOLO\n",
        "\n",
        "\n",
        "# ======= 修改成你的實際路徑 =======\n",
        "TRAINING_IMAGE = r\"C:\\Users\\307\\Desktop\\aicup\\training_image\"\n",
        "TRAINING_LABEL = r\"C:\\Users\\307\\Desktop\\aicup\\training_label\"\n",
        "ALIAS_ROOT     = r\"C:\\Users\\307\\Desktop\\aicup\\dataset\"   # 會建立 alias\\images 與 alias\\labels\n",
        "# =================================\n",
        "\n",
        "ALIAS_IMG = Path(ALIAS_ROOT) / \"images\"\n",
        "ALIAS_LBL = Path(ALIAS_ROOT) / \"labels\"\n",
        "Path(ALIAS_ROOT).mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "def safe_remove(p: Path):\n",
        "    if p.exists():\n",
        "        if p.is_symlink():\n",
        "            p.unlink()\n",
        "        elif p.is_dir():\n",
        "            # 先嘗試刪 junction（會當成目錄處理）\n",
        "            subprocess.run([\"cmd\", \"/c\", \"rmdir\", str(p)], stdout=subprocess.PIPE, stderr=subprocess.PIPE)\n",
        "\n",
        "def make_junction(link: Path, target: Path):\n",
        "    # 用 Windows 目錄連結（junction）取代 symlink，權限需求低\n",
        "    cmd = [\"cmd\", \"/c\", \"mklink\", \"/J\", str(link), str(target)]\n",
        "    ret = subprocess.run(cmd, stdout=subprocess.PIPE, stderr=subprocess.PIPE, text=True)\n",
        "    if ret.returncode != 0:\n",
        "        print(\"[錯誤] 建立目錄連結失敗：\", ret.stderr.strip())\n",
        "        print(\"請確認目標存在，或改用系統管理員執行。\")\n",
        "        sys.exit(1)\n",
        "\n",
        "def find_patient_root(root: Path) -> Path:\n",
        "    for dp, dns, _ in os.walk(root):\n",
        "        if any(d.startswith(\"patient\") for d in dns):\n",
        "            return Path(dp)\n",
        "    return root\n",
        "\n",
        "# 驗證來源資料夾\n",
        "for p in [TRAINING_IMAGE, TRAINING_LABEL]:\n",
        "    if not Path(p).is_dir():\n",
        "        raise FileNotFoundError(f\"找不到資料夾：{p}\")\n",
        "\n",
        "img_root = find_patient_root(Path(TRAINING_IMAGE))\n",
        "lbl_root = find_patient_root(Path(TRAINING_LABEL))\n",
        "print(\"IMG_ROOT =\", img_root)\n",
        "print(\"LBL_ROOT =\", lbl_root)\n",
        "\n",
        "# 清掉舊連結並建立新的 junction\n",
        "safe_remove(ALIAS_IMG); safe_remove(ALIAS_LBL)\n",
        "make_junction(ALIAS_IMG, img_root)\n",
        "make_junction(ALIAS_LBL, lbl_root)\n",
        "print(f\"[OK] 建立連結：{ALIAS_IMG} → {img_root}\")\n",
        "print(f\"[OK] 建立連結：{ALIAS_LBL} → {lbl_root}\")\n",
        "\n",
        "def write_list(p_start, p_end, out_txt):\n",
        "    total, kept = 0, 0\n",
        "    recs = []\n",
        "    for i in range(p_start, p_end + 1):\n",
        "        patient = f\"patient{i:04d}\"\n",
        "        img_dir = ALIAS_IMG / patient\n",
        "        lbl_dir = ALIAS_LBL / patient\n",
        "        if not img_dir.is_dir() or not lbl_dir.is_dir():\n",
        "            continue\n",
        "        for img_path in glob.glob(str(img_dir / \"*.png\")):\n",
        "            total += 1\n",
        "            base = Path(img_path).stem\n",
        "            lbl_path = lbl_dir / f\"{base}.txt\"\n",
        "            if lbl_path.exists() and lbl_path.stat().st_size > 0:\n",
        "                recs.append(img_path)   # 注意：清單中保留的是「images」路徑\n",
        "                kept += 1\n",
        "    recs.sort()\n",
        "    Path(out_txt).write_text(\"\\n\".join(recs), encoding=\"utf-8\")\n",
        "    print(f\"[OK] {out_txt}：影像{total}，有效配對{kept}\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "專案根目錄： c:\\Users\\307\\Desktop\\aicup\n",
            "影像： C:\\Users\\307\\Desktop\\aicup\\dataset\\images\n",
            "標註： C:\\Users\\307\\Desktop\\aicup\\dataset\\labels\n"
          ]
        }
      ],
      "source": [
        "# 1) 設定專案、資料、輸出目錄\n",
        "# 2) 鎖定隨機性（可重現）\n",
        "# 3) 檢查必要套件（ultralytics）是否可用\n",
        "import os\n",
        "import random\n",
        "import json\n",
        "import math\n",
        "from pathlib import Path\n",
        "from collections import defaultdict\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "# --- 可自訂參數（依你的環境調整） ---\n",
        "PROJECT_ROOT = Path.cwd()                 # 預設為當前 Notebook 所在目錄\n",
        "IMAGES_DIR   = r\"C:\\Users\\307\\Desktop\\aicup\\dataset\\images\"       # 影像：*.png（512x512）\n",
        "LABELS_DIR   = r\"C:\\Users\\307\\Desktop\\aicup\\dataset\\labels\"       # 標註：YOLO .txt（同名）\n",
        "FOLDS_DIR    = PROJECT_ROOT / \"folds\"     # 這次新增的 5-fold 清單輸出目錄\n",
        "RUNS_DIR     = PROJECT_ROOT / \"runs\"      # 訓練輸出（權重、log）\n",
        "SUBMIT_DIR   = PROJECT_ROOT / \"submissions\"\n",
        "YAML_DIR   = PROJECT_ROOT / \"fold_yaml\"  #  fold 專用 data.yaml 目錄\n",
        "\n",
        "\n",
        "# ultralytics（YOLO）若沒裝，請先安裝：pip install ultralytics\n",
        "try:\n",
        "    from ultralytics import YOLO\n",
        "    _yolo_ok = True\n",
        "except Exception as e:\n",
        "    print(\"⚠️ 未偵測到 ultralytics，訓練/推論區塊會需要它。\", e)\n",
        "    _yolo_ok = False\n",
        "\n",
        "# --- 亂數種子：鎖定所有主要來源 ---\n",
        "SEED = 42\n",
        "random.seed(SEED); np.random.seed(SEED)\n",
        "\n",
        "import torch\n",
        "torch.manual_seed(SEED)\n",
        "torch.cuda.manual_seed_all(SEED)\n",
        "torch.backends.cudnn.deterministic = True  # 較穩定但可能略慢\n",
        "torch.backends.cudnn.benchmark = False\n",
        "\n",
        "# --- 目錄建立 ---\n",
        "for d in [FOLDS_DIR, RUNS_DIR, SUBMIT_DIR, YAML_DIR]:\n",
        "    d.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "print(\"專案根目錄：\", PROJECT_ROOT)\n",
        "print(\"影像：\", IMAGES_DIR)\n",
        "print(\"標註：\", LABELS_DIR)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "總影像數： 16863\n",
            "總正樣本數： 2787\n",
            "病人數： 50\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>patient_id</th>\n",
              "      <th>n_images</th>\n",
              "      <th>n_pos</th>\n",
              "      <th>pos_ratio</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>patient0001</td>\n",
              "      <td>341</td>\n",
              "      <td>50</td>\n",
              "      <td>0.146628</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>patient0002</td>\n",
              "      <td>391</td>\n",
              "      <td>65</td>\n",
              "      <td>0.166240</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>patient0003</td>\n",
              "      <td>324</td>\n",
              "      <td>70</td>\n",
              "      <td>0.216049</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>patient0004</td>\n",
              "      <td>365</td>\n",
              "      <td>71</td>\n",
              "      <td>0.194521</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>patient0005</td>\n",
              "      <td>285</td>\n",
              "      <td>54</td>\n",
              "      <td>0.189474</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>patient0006</td>\n",
              "      <td>277</td>\n",
              "      <td>47</td>\n",
              "      <td>0.169675</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>patient0007</td>\n",
              "      <td>389</td>\n",
              "      <td>65</td>\n",
              "      <td>0.167095</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>patient0008</td>\n",
              "      <td>287</td>\n",
              "      <td>38</td>\n",
              "      <td>0.132404</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>patient0009</td>\n",
              "      <td>298</td>\n",
              "      <td>42</td>\n",
              "      <td>0.140940</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>patient0010</td>\n",
              "      <td>280</td>\n",
              "      <td>62</td>\n",
              "      <td>0.221429</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "    patient_id  n_images  n_pos  pos_ratio\n",
              "0  patient0001       341     50   0.146628\n",
              "1  patient0002       391     65   0.166240\n",
              "2  patient0003       324     70   0.216049\n",
              "3  patient0004       365     71   0.194521\n",
              "4  patient0005       285     54   0.189474\n",
              "5  patient0006       277     47   0.169675\n",
              "6  patient0007       389     65   0.167095\n",
              "7  patient0008       287     38   0.132404\n",
              "8  patient0009       298     42   0.140940\n",
              "9  patient0010       280     62   0.221429"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "# === 建立影像中繼資訊表 ===\n",
        "# 假設影像命名規則可解析出 patient_id 與 slice_idx\n",
        "# 常見命名範例：\n",
        "#   images/patient0001/000123.png  -> patient_id = patient0001, slice_idx = 123\n",
        "# 若你的命名不同，改寫 parse 函式即可。\n",
        "\n",
        "from typing import Tuple\n",
        "\n",
        "def parse_patient_and_slice(p: Path) -> Tuple[str, int]:\n",
        "    \"\"\"\n",
        "    傳回 (patient_id, slice_idx)\n",
        "    - patient_id: 父資料夾名或檔名中可辨識的段\n",
        "    - slice_idx: 從檔名取整數（不含副檔名）\n",
        "    \"\"\"\n",
        "    # 方案A：以父資料夾為病人ID、檔名（去副檔名）為切片index\n",
        "    patient_id = p.parent.name\n",
        "    try:\n",
        "        slice_idx = int(p.stem)\n",
        "    except:\n",
        "        # 若檔名不是純數字，可自訂解析規則\n",
        "        # 例如 \"CT_00123.png\" -> 提取 \"00123\"\n",
        "        digits = ''.join(ch for ch in p.stem if ch.isdigit())\n",
        "        slice_idx = int(digits) if digits else -1\n",
        "    return patient_id, slice_idx\n",
        "\n",
        "def has_label_for(image_path: Path) -> bool:\n",
        "    \"\"\"\n",
        "    YOLO 規則：標註檔與影像同名、位於 labels/ 對應層級。\n",
        "    例：images/patient0001/000123.png -> labels/patient0001/000123.txt\n",
        "    檢查該 .txt 是否存在且至少有一行（存在bbox）\n",
        "    \"\"\"\n",
        "    rel = image_path.relative_to(IMAGES_DIR).with_suffix(\".txt\")\n",
        "    label_path = LABELS_DIR / rel\n",
        "    if not label_path.exists():\n",
        "        return False\n",
        "    try:\n",
        "        # 至少一行標註才算「正樣本」\n",
        "        return label_path.read_text(encoding=\"utf-8\").strip() != \"\"\n",
        "    except:\n",
        "        return False\n",
        "\n",
        "# 確保 IMAGES_DIR 和 LABELS_DIR 是 Path 物件\n",
        "IMAGES_DIR = Path(IMAGES_DIR)\n",
        "LABELS_DIR = Path(LABELS_DIR)\n",
        "\n",
        "# 掃描所有影像\n",
        "all_images = sorted(IMAGES_DIR.rglob(\"*.png\"))\n",
        "rows = []\n",
        "for img in all_images:\n",
        "    pid, sidx = parse_patient_and_slice(img)\n",
        "    rows.append({\n",
        "        \"img_path\": str(img),\n",
        "        \"patient_id\": pid,\n",
        "        \"slice_idx\": sidx,\n",
        "        \"has_label\": has_label_for(img)\n",
        "    })\n",
        "df = pd.DataFrame(rows).sort_values([\"patient_id\", \"slice_idx\"]).reset_index(drop=True)\n",
        "\n",
        "# 基本統計\n",
        "per_patient = df.groupby(\"patient_id\").agg(\n",
        "    n_images=(\"img_path\", \"count\"),\n",
        "    n_pos=(\"has_label\", \"sum\")\n",
        ").reset_index()\n",
        "per_patient[\"pos_ratio\"] = per_patient[\"n_pos\"] / per_patient[\"n_images\"]\n",
        "\n",
        "print(\"總影像數：\", len(df))\n",
        "print(\"總正樣本數：\", df[\"has_label\"].sum())\n",
        "print(\"病人數：\", df[\"patient_id\"].nunique())\n",
        "display(per_patient.head(10))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "✅ 已建立 5-fold 病人級分配與基本清單\n"
          ]
        }
      ],
      "source": [
        "# ===　建立病人級 5-fold ===\n",
        "# 用 KFold 對「病人ID」做分割，每個 fold 互斥的病人集合。\n",
        "\n",
        "from sklearn.model_selection import KFold\n",
        "\n",
        "patient_ids = sorted(df[\"patient_id\"].unique().tolist())\n",
        "assert len(patient_ids) == 50, \"⚠️ 病人數應為 50，請檢查資料或命名解析。\"\n",
        "\n",
        "kf = KFold(n_splits=5, shuffle=True, random_state=SEED)\n",
        "fold_assign = {}  # fold_id -> {\"train\": [...], \"val\": [...]}\n",
        "\n",
        "for fold_id, (tr_idx, va_idx) in enumerate(kf.split(patient_ids), start=1):\n",
        "    tr_patients = [patient_ids[i] for i in tr_idx]\n",
        "    va_patients = [patient_ids[i] for i in va_idx]\n",
        "    fold_assign[fold_id] = {\"train\": tr_patients, \"val\": va_patients}\n",
        "\n",
        "# 儲存 JSON 以利重現\n",
        "with open(FOLDS_DIR / \"patient_folds.json\", \"w\", encoding=\"utf-8\") as f:\n",
        "    json.dump(fold_assign, f, ensure_ascii=False, indent=2)\n",
        "\n",
        "# 依病人ID，輸出各 fold 的原始（未擴充）train/val 影像清單\n",
        "for fold_id, dct in fold_assign.items():\n",
        "    tr = df[df[\"patient_id\"].isin(dct[\"train\"])]\n",
        "    va = df[df[\"patient_id\"].isin(dct[\"val\"])]\n",
        "    (FOLDS_DIR / f\"fold{fold_id}_train.txt\").write_text(\"\\n\".join(tr[\"img_path\"]), encoding=\"utf-8\")\n",
        "    (FOLDS_DIR / f\"fold{fold_id}_val.txt\").write_text(\"\\n\".join(va[\"img_path\"]), encoding=\"utf-8\")\n",
        "\n",
        "print(\"✅ 已建立 5-fold 病人級分配與基本清單\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>fold</th>\n",
              "      <th>n_train_total</th>\n",
              "      <th>n_pos_all</th>\n",
              "      <th>n_pos_used</th>\n",
              "      <th>n_near_cands</th>\n",
              "      <th>n_mid_cands</th>\n",
              "      <th>n_far_cands</th>\n",
              "      <th>n_easy_pool</th>\n",
              "      <th>n_near_used</th>\n",
              "      <th>n_mid_used</th>\n",
              "      <th>n_far_used</th>\n",
              "      <th>n_easy_used</th>\n",
              "      <th>n_rand_fallback_used</th>\n",
              "      <th>n_neg_target</th>\n",
              "      <th>n_neg_used</th>\n",
              "      <th>n_final_train</th>\n",
              "      <th>neg_pos_ratio_final</th>\n",
              "      <th>n_shortfall</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1</td>\n",
              "      <td>13820</td>\n",
              "      <td>2251</td>\n",
              "      <td>2251</td>\n",
              "      <td>320</td>\n",
              "      <td>640</td>\n",
              "      <td>9969</td>\n",
              "      <td>640</td>\n",
              "      <td>320</td>\n",
              "      <td>640</td>\n",
              "      <td>2701</td>\n",
              "      <td>640</td>\n",
              "      <td>2452</td>\n",
              "      <td>6753</td>\n",
              "      <td>6753</td>\n",
              "      <td>9004</td>\n",
              "      <td>3.0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>2</td>\n",
              "      <td>13266</td>\n",
              "      <td>2152</td>\n",
              "      <td>2152</td>\n",
              "      <td>320</td>\n",
              "      <td>640</td>\n",
              "      <td>9514</td>\n",
              "      <td>640</td>\n",
              "      <td>320</td>\n",
              "      <td>640</td>\n",
              "      <td>2583</td>\n",
              "      <td>640</td>\n",
              "      <td>2273</td>\n",
              "      <td>6456</td>\n",
              "      <td>6456</td>\n",
              "      <td>8608</td>\n",
              "      <td>3.0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>3</td>\n",
              "      <td>13551</td>\n",
              "      <td>2250</td>\n",
              "      <td>2250</td>\n",
              "      <td>320</td>\n",
              "      <td>640</td>\n",
              "      <td>9701</td>\n",
              "      <td>640</td>\n",
              "      <td>320</td>\n",
              "      <td>640</td>\n",
              "      <td>2700</td>\n",
              "      <td>640</td>\n",
              "      <td>2450</td>\n",
              "      <td>6750</td>\n",
              "      <td>6750</td>\n",
              "      <td>9000</td>\n",
              "      <td>3.0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>4</td>\n",
              "      <td>13344</td>\n",
              "      <td>2232</td>\n",
              "      <td>2232</td>\n",
              "      <td>320</td>\n",
              "      <td>640</td>\n",
              "      <td>9512</td>\n",
              "      <td>640</td>\n",
              "      <td>320</td>\n",
              "      <td>640</td>\n",
              "      <td>2679</td>\n",
              "      <td>640</td>\n",
              "      <td>2417</td>\n",
              "      <td>6696</td>\n",
              "      <td>6696</td>\n",
              "      <td>8928</td>\n",
              "      <td>3.0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>5</td>\n",
              "      <td>13471</td>\n",
              "      <td>2263</td>\n",
              "      <td>2263</td>\n",
              "      <td>320</td>\n",
              "      <td>640</td>\n",
              "      <td>9608</td>\n",
              "      <td>640</td>\n",
              "      <td>320</td>\n",
              "      <td>640</td>\n",
              "      <td>2715</td>\n",
              "      <td>640</td>\n",
              "      <td>2474</td>\n",
              "      <td>6789</td>\n",
              "      <td>6789</td>\n",
              "      <td>9052</td>\n",
              "      <td>3.0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   fold  n_train_total  n_pos_all  n_pos_used  n_near_cands  n_mid_cands  \\\n",
              "0     1          13820       2251        2251           320          640   \n",
              "1     2          13266       2152        2152           320          640   \n",
              "2     3          13551       2250        2250           320          640   \n",
              "3     4          13344       2232        2232           320          640   \n",
              "4     5          13471       2263        2263           320          640   \n",
              "\n",
              "   n_far_cands  n_easy_pool  n_near_used  n_mid_used  n_far_used  n_easy_used  \\\n",
              "0         9969          640          320         640        2701          640   \n",
              "1         9514          640          320         640        2583          640   \n",
              "2         9701          640          320         640        2700          640   \n",
              "3         9512          640          320         640        2679          640   \n",
              "4         9608          640          320         640        2715          640   \n",
              "\n",
              "   n_rand_fallback_used  n_neg_target  n_neg_used  n_final_train  \\\n",
              "0                  2452          6753        6753           9004   \n",
              "1                  2273          6456        6456           8608   \n",
              "2                  2450          6750        6750           9000   \n",
              "3                  2417          6696        6696           8928   \n",
              "4                  2474          6789        6789           9052   \n",
              "\n",
              "   neg_pos_ratio_final  n_shortfall  \n",
              "0                  3.0            0  \n",
              "1                  3.0            0  \n",
              "2                  3.0            0  \n",
              "3                  3.0            0  \n",
              "4                  3.0            0  "
            ]
          },
          "execution_count": 11,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# === 區塊 3（取代版）：為訓練集建立「三圈帶」難負樣本（Near / Mid / Far） ===\n",
        "# 目的：\n",
        "# 1) 利用你的觀察：「正樣本多成段（~50 張），常落在 slice 100–350」這個先驗，\n",
        "#    將「難負樣本」重點放在「段落邊界附近」，提升模型對細微差異的鑑別力。\n",
        "# 2) 將負樣本依三圈帶分級抽樣（Near > Mid > Far），並維持整體「負:正 ≈ TARGET_NEG_POS_RATIO : 1」。\n",
        "# 3) 只對「訓練集」做擴充；驗證/測試不動，避免高估泛化。\n",
        "#\n",
        "# 重要輸出（與原流程一致）：\n",
        "# - folds/fold{K}_train_neighbors.txt\n",
        "#\n",
        "# 名詞：\n",
        "# - 正段落（run）：同病人的 has_label=True 的 slice_idx 連續區間（例：121..172）。\n",
        "# - 三圈帶（對每個 run）：\n",
        "#   Near：緊貼邊界 ±1..3            → 最難負樣本（最像但無標註）\n",
        "#   Mid： 邊界外側 ±4..10（此處擴到 ±15 以更充足） → 中難負樣本\n",
        "#   Far： 距離任一 run 邊界 > FAR_GAP（預設 30） → 容易負樣本（補數用）\n",
        "#\n",
        "# 補足策略（你剛要求的）：\n",
        "#   Near/Mid/Far 都抽完後，若仍不足 → 先用 easy_all_neg 補，再用「整個訓練集中所有無標註切片」隨機補到目標數。\n",
        "#\n",
        "# 注意：\n",
        "# - 這個區塊假設前面區塊已建立 df（含 img_path, patient_id, slice_idx, has_label）以及 fold_assign。\n",
        "# - 輸出檔名與介面不變，後續區塊 4/4b/5/6/7/8 無需修改。\n",
        "\n",
        "from collections import defaultdict\n",
        "import random\n",
        "\n",
        "# --- 可調參數（建議起手值） ---\n",
        "TARGET_NEG_POS_RATIO = 3         # 整體「負:正」目標比例（建議 2.0 ~ 3.0）\n",
        "NEAR_OFFSETS = (1, 2, 3, 4)           # Near：貼邊 ±1..3\n",
        "# 將 Mid 擴至 ±4..15，避免候選不足（你可改回 ±4..10 視資料量）\n",
        "MID_OFFSETS  = tuple(range(5, 13)) # Mid：±4..10\n",
        "FAR_GAP      = 20                # Far：距離任一正段落邊界 > 20 張\n",
        "\n",
        "# 三圈帶目標比例（和為 1.0）\n",
        "BAND_RATIOS = {\n",
        "    \"near\": 0.2,\n",
        "    \"mid\":  0.4,\n",
        "    \"far\":  0.4,\n",
        "}\n",
        "\n",
        "def _find_positive_runs(pdf):\n",
        "    \"\"\"\n",
        "    將單一病人的正樣本（has_label=True）的 slice_idx 依連續性切成多個 run。\n",
        "    回傳：runs = [(start_idx, end_idx), ...]（皆為包含端點的整數）\n",
        "    \"\"\"\n",
        "    pos_idx = sorted(pdf.loc[pdf[\"has_label\"], \"slice_idx\"].tolist())\n",
        "    runs = []\n",
        "    if not pos_idx:\n",
        "        return runs\n",
        "    start = prev = pos_idx[0]\n",
        "    for s in pos_idx[1:]:\n",
        "        if s == prev + 1:\n",
        "            prev = s\n",
        "        else:\n",
        "            runs.append((start, prev))\n",
        "            start = prev = s\n",
        "    runs.append((start, prev))\n",
        "    return runs\n",
        "\n",
        "def _band_candidates_for_patient(pdf):\n",
        "    \"\"\"\n",
        "    對單一病人產生三圈帶候選負樣本。\n",
        "    參數：\n",
        "      - pdf：該病人在「訓練集」的 DataFrame（欄位需含 has_label/slice_idx/img_path）\n",
        "    回傳：\n",
        "      dict = {\n",
        "        \"near\": [paths...], \"mid\": [paths...], \"far\": [paths...],\n",
        "        \"easy_all_neg\": [paths...]  # 該病人所有無標註負樣本全集（最終補數用）\n",
        "      }\n",
        "    \"\"\"\n",
        "    runs = _find_positive_runs(pdf)\n",
        "    # 該病人所有無標註的切片（全集），作為最後補數來源\n",
        "    easy_all_neg = set(pdf.loc[~pdf[\"has_label\"], \"img_path\"].tolist())\n",
        "\n",
        "    # 若沒有任何正樣本，該病人無 run，全部都算 easy（不產生 near/mid/far）\n",
        "    if not runs:\n",
        "        return {\"near\": [], \"mid\": [], \"far\": [], \"easy_all_neg\": sorted(easy_all_neg)}\n",
        "\n",
        "    # 準備索引，便於用 slice_idx 快速取得該列資料\n",
        "    sidx_to_row = {\n",
        "        int(r.slice_idx): r\n",
        "        for r in pdf[[\"slice_idx\",\"img_path\",\"has_label\"]].itertuples(index=False)\n",
        "    }\n",
        "    near_set, mid_set, far_set = set(), set(), set()\n",
        "\n",
        "    # 1) Near / Mid：以每個 run 的兩側邊界向外擴散收集（只收無標註）\n",
        "    for (st, ed) in runs:\n",
        "        for off in NEAR_OFFSETS:\n",
        "            # 左邊界外側\n",
        "            idx = st - off\n",
        "            if idx in sidx_to_row and not sidx_to_row[idx].has_label:\n",
        "                near_set.add(sidx_to_row[idx].img_path)\n",
        "            # 右邊界外側\n",
        "            idx = ed + off\n",
        "            if idx in sidx_to_row and not sidx_to_row[idx].has_label:\n",
        "                near_set.add(sidx_to_row[idx].img_path)\n",
        "        for off in MID_OFFSETS:\n",
        "            idx = st - off\n",
        "            if idx in sidx_to_row and not sidx_to_row[idx].has_label:\n",
        "                mid_set.add(sidx_to_row[idx].img_path)\n",
        "            idx = ed + off\n",
        "            if idx in sidx_to_row and not sidx_to_row[idx].has_label:\n",
        "                mid_set.add(sidx_to_row[idx].img_path)\n",
        "\n",
        "    # 2) Far：距離所有 run 邊界 > FAR_GAP 的無標註切片\n",
        "    #    邊界定義為每個 run 的起/迄（st, ed）\n",
        "    boundaries = []\n",
        "    for (st, ed) in runs:\n",
        "        boundaries.extend([st, ed])\n",
        "    boundaries = sorted(boundaries)\n",
        "\n",
        "    def _dist_to_boundary(s):\n",
        "        # 回傳該 slice_idx 到最近邊界之距離（張數）\n",
        "        return min(abs(s - b) for b in boundaries) if boundaries else 10**9\n",
        "\n",
        "    # 修正版（正確解包 3 欄並直接使用 img_path）：\n",
        "    for s, img_path, has_label in pdf.loc[:, [\"slice_idx\",\"img_path\",\"has_label\"]].itertuples(index=False, name=None):\n",
        "        if (not has_label) and _dist_to_boundary(int(s)) > FAR_GAP:\n",
        "            far_set.add(img_path)\n",
        "\n",
        "    return {\n",
        "        \"near\": sorted(near_set),\n",
        "        \"mid\":  sorted(mid_set),\n",
        "        \"far\":  sorted(far_set),\n",
        "        \"easy_all_neg\": sorted(easy_all_neg)\n",
        "    }\n",
        "\n",
        "def build_neighbors_for_fold(fold_id: int):\n",
        "    \"\"\"\n",
        "    以「三圈帶」策略為該 fold 構建訓練清單：\n",
        "    - 正樣本：全納入（所有 has_label=True 的切片）\n",
        "    - 負樣本：按 Near -> Mid -> Far 的目標比例抽樣到目標總數；若不足，先用 easy_all_neg 補；\n",
        "              再不足則用「整個訓練集的所有無標註切片」隨機補到目標數（避免只侷限單病人的 easy 池）。\n",
        "    - 產出：folds/fold{fold_id}_train_neighbors.txt\n",
        "    - 回傳：統計 dict（檢視比例與是否有短缺）\n",
        "    \"\"\"\n",
        "    # 取該 fold 的 train/val 病人\n",
        "    tr_p = set(fold_assign[fold_id][\"train\"])\n",
        "    va_p = set(fold_assign[fold_id][\"val\"])\n",
        "\n",
        "    # 拆出 train / val DataFrame（僅 train 擴充）\n",
        "    dtr = df[df[\"patient_id\"].isin(tr_p)].copy()\n",
        "    dva = df[df[\"patient_id\"].isin(va_p)].copy()\n",
        "\n",
        "    # 以病人建立三圈帶候選集合\n",
        "    near_cands, mid_cands, far_cands = [], [], []\n",
        "    easy_all_neg_union = set()\n",
        "    for pid, pdf in dtr.groupby(\"patient_id\"):\n",
        "        pdf = pdf.sort_values(\"slice_idx\").reset_index(drop=True)\n",
        "        bands = _band_candidates_for_patient(pdf)\n",
        "        near_cands.extend(bands[\"near\"])\n",
        "        mid_cands.extend(bands[\"mid\"])\n",
        "        far_cands.extend(bands[\"far\"])\n",
        "        easy_all_neg_union.update(bands[\"easy_all_neg\"])\n",
        "\n",
        "    # 去重 & 彼此去重（避免交集）\n",
        "    near_cands = sorted(set(near_cands))\n",
        "    mid_cands  = sorted(set(mid_cands) - set(near_cands))\n",
        "    far_cands  = sorted(set(far_cands) - set(near_cands) - set(mid_cands))\n",
        "    # easy 池 = 所有無標註 - (near|mid|far)\n",
        "    easy_all_neg = sorted(easy_all_neg_union - set(near_cands) - set(mid_cands) - set(far_cands))\n",
        "\n",
        "    # 正樣本：全納入\n",
        "    pos_rows = dtr[dtr[\"has_label\"]].copy()\n",
        "    selected_pos = pos_rows[\"img_path\"].tolist()\n",
        "    n_pos = len(selected_pos)\n",
        "\n",
        "    # 目標負樣本總數\n",
        "    n_neg_target = int(TARGET_NEG_POS_RATIO * n_pos)\n",
        "\n",
        "    # 依比例分配各圈帶目標數\n",
        "    target_near = int(round(BAND_RATIOS[\"near\"] * n_neg_target))\n",
        "    target_mid  = int(round(BAND_RATIOS[\"mid\"]  * n_neg_target))\n",
        "    target_far  = n_neg_target - target_near - target_mid  # 收尾防四捨五入誤差\n",
        "\n",
        "    # 小工具：隨機抽樣且不超界\n",
        "    def _sample(lst, k):\n",
        "        if k <= 0 or not lst:\n",
        "            return []\n",
        "        lst = lst[:]  # copy\n",
        "        random.shuffle(lst)\n",
        "        return lst[:min(k, len(lst))]\n",
        "\n",
        "    # 先照 Near -> Mid -> Far 抽樣\n",
        "    sel_near = _sample(near_cands, target_near)\n",
        "    remaining = n_neg_target - len(sel_near)\n",
        "\n",
        "    sel_mid  = _sample(mid_cands, min(target_mid, remaining))\n",
        "    remaining = n_neg_target - len(sel_near) - len(sel_mid)\n",
        "\n",
        "    sel_far  = _sample(far_cands, min(target_far, remaining))\n",
        "    remaining = n_neg_target - len(sel_near) - len(sel_mid) - len(sel_far)\n",
        "\n",
        "    # 第一步補：用 easy_all_neg 補\n",
        "    sel_easy = _sample(easy_all_neg, remaining)\n",
        "    selected_negs = sel_near + sel_mid + sel_far + sel_easy\n",
        "    remaining = n_neg_target - len(selected_negs)\n",
        "\n",
        "    # 最終補：用「整個訓練集的所有無標註切片」隨機補（扣掉已選，確保不重複）\n",
        "    if remaining > 0:\n",
        "        all_neg_pool = sorted(set(dtr.loc[~dtr[\"has_label\"], \"img_path\"].tolist()))\n",
        "        all_neg_left = sorted(set(all_neg_pool) - set(selected_negs))\n",
        "        sel_rand_fallback = _sample(all_neg_left, remaining)\n",
        "    else:\n",
        "        sel_rand_fallback = []\n",
        "\n",
        "    selected_negs += sel_rand_fallback\n",
        "    remaining = n_neg_target - len(selected_negs)  # 若 still >0，代表資料本身不足（可忽略）\n",
        "\n",
        "    # 合併最終 train 清單並打散\n",
        "    final_train = selected_pos + selected_negs\n",
        "    random.shuffle(final_train)\n",
        "\n",
        "    # 輸出檔案（檔名維持不變）\n",
        "    out_path = FOLDS_DIR / f\"fold{fold_id}_train_neighbors.txt\"\n",
        "    out_path.write_text(\"\\n\".join(final_train), encoding=\"utf-8\")\n",
        "\n",
        "    # 統計回傳（含各圈帶使用量與最終是否仍短缺）\n",
        "    stats = {\n",
        "        \"fold\": fold_id,\n",
        "        \"n_train_total\": len(dtr),\n",
        "        \"n_pos_all\": int(dtr[\"has_label\"].sum()),\n",
        "        \"n_pos_used\": len(selected_pos),\n",
        "\n",
        "        \"n_near_cands\": len(near_cands),\n",
        "        \"n_mid_cands\": len(mid_cands),\n",
        "        \"n_far_cands\": len(far_cands),\n",
        "        \"n_easy_pool\": len(easy_all_neg),\n",
        "\n",
        "        \"n_near_used\": len(sel_near),\n",
        "        \"n_mid_used\": len(sel_mid),\n",
        "        \"n_far_used\": len(sel_far),\n",
        "        \"n_easy_used\": len(sel_easy),\n",
        "        \"n_rand_fallback_used\": len(sel_rand_fallback),\n",
        "\n",
        "        \"n_neg_target\": n_neg_target,\n",
        "        \"n_neg_used\": len(selected_negs),\n",
        "        \"n_final_train\": len(final_train),\n",
        "        \"neg_pos_ratio_final\": round(len(selected_negs) / max(1, len(selected_pos)), 3),\n",
        "        \"n_shortfall\": max(0, remaining)  # 若仍不足，>0；代表資料上限，不是錯誤\n",
        "    }\n",
        "    return stats\n",
        "\n",
        "# 逐 fold 生成新版三圈帶清單與統計（與原流程相同）\n",
        "all_stats = []\n",
        "for k in range(1, 6):\n",
        "    stats_k = build_neighbors_for_fold(k)\n",
        "    all_stats.append(stats_k)\n",
        "\n",
        "pd.DataFrame(all_stats)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "##  訓練模型"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "✅ 資料 YAML 會在每個 fold 動態產生\n"
          ]
        }
      ],
      "source": [
        "# === 資料 YAML 覆寫策略 ===\n",
        "# 你的 aortic_val.yaml 依然存在，但 train/val 清單由 Notebook 動態替換。\n",
        "# 這裡組裝一個臨時 YAML 字串，供 YOLO 調用。\n",
        "\n",
        "import yaml\n",
        "\n",
        "def make_data_yaml(train_list_path: Path, val_list_path: Path) -> Path:\n",
        "    data_dict = {\n",
        "        \"path\": str(r\"C:\\Users\\307\\Desktop\\aicup\\dataset\"),    # 資料根目錄（可讓YOLO拼相對路徑）\n",
        "        \"train\": str(train_list_path),\n",
        "        \"val\": str(val_list_path),\n",
        "        \"nc\": 1,\n",
        "        \"names\": {0: \"aortic_valve\"}\n",
        "    }\n",
        "    tmp_yaml = YAML_DIR / f\"{train_list_path.stem}.yaml\"\n",
        "    with open(tmp_yaml, \"w\", encoding=\"utf-8\") as f:\n",
        "        yaml.safe_dump(data_dict, f, allow_unicode=True, sort_keys=False)\n",
        "    return tmp_yaml\n",
        "\n",
        "print(\"✅ 資料 YAML 會在每個 fold 動態產生\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {},
      "outputs": [],
      "source": [
        "# === 訓練全域參數  ===\n",
        "AUG_ARGS = dict(\n",
        "    # degrees=5,            # ±5°\n",
        "    # translate=0.05,       # 5% 平移\n",
        "    scale=0.2,            # 0.9~1.1 縮放\n",
        "    # shear=0.03,           # 輕剪切\n",
        "    fliplr=0.0,           # 關水平翻轉（若驗證無害再做 A/B：≤0.1）\n",
        "    flipud=0.0,           # 關垂直翻轉\n",
        "    mosaic=0.0,           # 關 mosaic\n",
        "    mixup=0.0,            # 關 mixup\n",
        "    hsv_h=0.0,            # 關 HSV（CT 灰階）\n",
        "    hsv_s=0.0,\n",
        "    hsv_v=0.0\n",
        ")\n",
        "\n",
        "# —— 統一 train() 參數包 ——（含增強）\n",
        "TRAIN_ARGS = dict(\n",
        "    imgsz=640,\n",
        "    epochs=40,\n",
        "    warmup_epochs=3.0,    # 適度 warmup\n",
        "    batch=8,\n",
        "    lr0=5e-4,\n",
        "    weight_decay=5e-4,\n",
        "    cos_lr=True,          # Cosine LR\n",
        "    amp=True,\n",
        "    patience=15,          # 早停\n",
        "    device=0 ,\n",
        "    optimizer='AdamW',\n",
        "    cache=\"disk\",    # 可加速但佔空間\n",
        "    **AUG_ARGS\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "      23/40      6.56G     0.7513     0.4157      0.875          2        640: 100%|██████████| 1132/1132 [05:35<00:00,  3.37it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 212/212 [00:30<00:00,  6.92it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "                   all       3392        524      0.926      0.857      0.931      0.693\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "      24/40      6.56G     0.7395     0.3918      0.878          1        640: 100%|██████████| 1132/1132 [05:30<00:00,  3.43it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 212/212 [00:29<00:00,  7.08it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "                   all       3392        524        0.9      0.863      0.921      0.669\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "      25/40       6.6G       0.73     0.3863      0.872          0        640: 100%|██████████| 1132/1132 [05:28<00:00,  3.45it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 212/212 [00:30<00:00,  7.05it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "                   all       3392        524      0.911      0.857      0.925      0.672\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "      26/40      6.55G      0.715     0.3698     0.8649          1        640: 100%|██████████| 1132/1132 [05:32<00:00,  3.41it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 212/212 [00:30<00:00,  6.86it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "                   all       3392        524        0.9      0.847      0.927      0.692\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "      27/40       6.6G     0.6816     0.3552     0.8569          2        640: 100%|██████████| 1132/1132 [05:28<00:00,  3.45it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 212/212 [00:29<00:00,  7.09it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "                   all       3392        524      0.917      0.872      0.932      0.694\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "      28/40      6.56G     0.6826     0.3575     0.8541          1        640: 100%|██████████| 1132/1132 [05:28<00:00,  3.45it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 212/212 [00:29<00:00,  7.09it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "                   all       3392        524      0.933      0.851      0.927      0.694\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "      29/40      6.59G     0.6728     0.3454     0.8462          1        640: 100%|██████████| 1132/1132 [05:28<00:00,  3.44it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 212/212 [00:29<00:00,  7.11it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "                   all       3392        524       0.92      0.863      0.923       0.69\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "      30/40      6.56G     0.6384       0.33     0.8355          2        640: 100%|██████████| 1132/1132 [05:28<00:00,  3.45it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 212/212 [00:29<00:00,  7.09it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "                   all       3392        524       0.93      0.844      0.914      0.688\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Closing dataloader mosaic\n",
            "\u001b[34m\u001b[1malbumentations: \u001b[0mBlur(p=0.01, blur_limit=(3, 7)), MedianBlur(p=0.01, blur_limit=(3, 7)), ToGray(p=0.01, method='weighted_average', num_output_channels=3), CLAHE(p=0.01, clip_limit=(1.0, 4.0), tile_grid_size=(8, 8))\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\307\\Desktop\\aicup\\.venv\\Lib\\site-packages\\ultralytics\\data\\augment.py:1853: UserWarning: Argument(s) 'quality_lower' are not valid for transform ImageCompression\n",
            "  A.ImageCompression(quality_lower=75, p=0.0),\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "      31/40      6.59G     0.6389     0.3243     0.8391          2        640: 100%|██████████| 1132/1132 [05:30<00:00,  3.43it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 212/212 [00:29<00:00,  7.08it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "                   all       3392        524      0.908      0.864      0.927      0.689\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "      32/40      6.55G     0.6237     0.3175     0.8326          1        640: 100%|██████████| 1132/1132 [05:28<00:00,  3.45it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 212/212 [00:30<00:00,  7.06it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "                   all       3392        524      0.915      0.863      0.926       0.69\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "      33/40      6.57G     0.6011     0.3024     0.8086          1        640: 100%|██████████| 1132/1132 [05:28<00:00,  3.45it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 212/212 [00:29<00:00,  7.10it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "                   all       3392        524       0.92      0.859      0.924      0.695\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "      34/40      6.57G     0.5938     0.3057     0.8224          0        640: 100%|██████████| 1132/1132 [05:28<00:00,  3.44it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 212/212 [00:29<00:00,  7.10it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "                   all       3392        524      0.913      0.841      0.914      0.683\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "      35/40       6.6G     0.5879     0.2965     0.8142          1        640: 100%|██████████| 1132/1132 [05:27<00:00,  3.45it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 212/212 [00:29<00:00,  7.09it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "                   all       3392        524       0.92      0.859      0.919      0.687\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "      36/40      6.56G     0.5752     0.2884     0.7972          0        640: 100%|██████████| 1132/1132 [05:28<00:00,  3.45it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 212/212 [00:29<00:00,  7.09it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "                   all       3392        524      0.907      0.863      0.921      0.688\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "      37/40      6.59G      0.574     0.2921     0.8042          0        640: 100%|██████████| 1132/1132 [05:27<00:00,  3.45it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 212/212 [00:29<00:00,  7.07it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "                   all       3392        524      0.912      0.863      0.924      0.693\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "      38/40      6.55G     0.5649     0.2869     0.7956          3        640: 100%|██████████| 1132/1132 [05:27<00:00,  3.45it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 212/212 [00:29<00:00,  7.10it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "                   all       3392        524      0.911      0.857      0.924      0.694\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "      39/40      6.59G     0.5599     0.2849     0.7948          1        640: 100%|██████████| 1132/1132 [05:28<00:00,  3.45it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 212/212 [00:30<00:00,  6.98it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "                   all       3392        524      0.906       0.86       0.92      0.691\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "      40/40      6.55G     0.5555     0.2812     0.7967          1        640: 100%|██████████| 1132/1132 [05:28<00:00,  3.45it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 212/212 [00:29<00:00,  7.07it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "                   all       3392        524      0.909      0.859       0.92       0.69\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "40 epochs completed in 4.040 hours.\n",
            "Optimizer stripped from c:\\Users\\307\\Desktop\\aicup\\runs\\fold5\\exp\\weights\\last.pt, 39.7MB\n",
            "Optimizer stripped from c:\\Users\\307\\Desktop\\aicup\\runs\\fold5\\exp\\weights\\best.pt, 39.7MB\n",
            "\n",
            "Validating c:\\Users\\307\\Desktop\\aicup\\runs\\fold5\\exp\\weights\\best.pt...\n",
            "Ultralytics 8.3.63  Python-3.12.10 torch-2.6.0+cu124 CUDA:0 (NVIDIA GeForce RTX 2080 Ti, 11264MiB)\n",
            "YOLOv12m summary (fused): 402 layers, 19,577,299 parameters, 0 gradients, 59.5 GFLOPs\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 212/212 [00:27<00:00,  7.74it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "                   all       3392        524      0.918      0.859      0.924      0.695\n",
            "Speed: 0.2ms preprocess, 6.8ms inference, 0.0ms loss, 0.3ms postprocess per image\n",
            "Results saved to \u001b[1mc:\\Users\\307\\Desktop\\aicup\\runs\\fold5\\exp\u001b[0m\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>fold</th>\n",
              "      <th>best_weight</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1</td>\n",
              "      <td>c:\\Users\\307\\Desktop\\aicup\\runs\\fold1\\exp\\weig...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>2</td>\n",
              "      <td>c:\\Users\\307\\Desktop\\aicup\\runs\\fold2\\exp\\weig...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>3</td>\n",
              "      <td>c:\\Users\\307\\Desktop\\aicup\\runs\\fold3\\exp\\weig...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>4</td>\n",
              "      <td>c:\\Users\\307\\Desktop\\aicup\\runs\\fold4\\exp\\weig...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>5</td>\n",
              "      <td>c:\\Users\\307\\Desktop\\aicup\\runs\\fold5\\exp\\weig...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   fold                                        best_weight\n",
              "0     1  c:\\Users\\307\\Desktop\\aicup\\runs\\fold1\\exp\\weig...\n",
              "1     2  c:\\Users\\307\\Desktop\\aicup\\runs\\fold2\\exp\\weig...\n",
              "2     3  c:\\Users\\307\\Desktop\\aicup\\runs\\fold3\\exp\\weig...\n",
              "3     4  c:\\Users\\307\\Desktop\\aicup\\runs\\fold4\\exp\\weig...\n",
              "4     5  c:\\Users\\307\\Desktop\\aicup\\runs\\fold5\\exp\\weig..."
            ]
          },
          "execution_count": 14,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# === 5-fold 訓練（帶入增強參數） ===\n",
        "# 說明：\n",
        "# - 僅需確認 model.train(...) 時有帶入 **TRAIN_ARGS**（其中包含我們的增強鍵）\n",
        "# - 其他流程（fold 清單、鄰近片難負樣本、輸出路徑）維持不變\n",
        "\n",
        "if not _yolo_ok:\n",
        "    raise SystemExit(\"請先安裝 ultralytics 才能執行訓練。\")\n",
        "\n",
        "fold_results = []\n",
        "for fold_id in range(1, 6):\n",
        "    train_list = FOLDS_DIR / f\"fold{fold_id}_train_neighbors.txt\"  # 使用「含鄰近片」的訓練清單\n",
        "    val_list   = FOLDS_DIR / f\"fold{fold_id}_val.txt\"\n",
        "    data_yaml  = make_data_yaml(train_list, val_list)\n",
        "\n",
        "    save_dir = RUNS_DIR / f\"fold{fold_id}\"\n",
        "    save_dir.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "    print(f\"\\n=== 開始訓練：Fold {fold_id} ===\")\n",
        "    print(\"Train 清單：\", train_list)\n",
        "    print(\"Val 清單：\", val_list)\n",
        "\n",
        "    # —— 關鍵：把 TRAIN_ARGS（含增強）一併傳入 —— #\n",
        "    MODEL_PATH = \"yolov12m.pt\"   # 依你資源可改 n/s/m/l/x\n",
        "    model = YOLO(MODEL_PATH)  #\n",
        "    res = model.train(\n",
        "        data=str(data_yaml),\n",
        "        project=str(save_dir),\n",
        "        name=\"exp\",\n",
        "        **TRAIN_ARGS\n",
        "    )\n",
        "\n",
        "    # 取得最佳權重（Ultralytics 會存到 runs 下的 exp*/weights/best.pt）\n",
        "    best_weight = save_dir / \"exp\" / \"weights\" / \"best.pt\"\n",
        "    if not best_weight.exists():\n",
        "        best_candidates = list(save_dir.rglob(\"best.pt\"))\n",
        "        best_weight = best_candidates[0] if best_candidates else None\n",
        "\n",
        "    fold_results.append({\n",
        "        \"fold\": fold_id,\n",
        "        \"best_weight\": str(best_weight) if best_weight else \"\"\n",
        "    })\n",
        "\n",
        "pd.DataFrame(fold_results)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 預測"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {},
      "outputs": [],
      "source": [
        "# import os\n",
        "# import shutil\n",
        "\n",
        "# src_root = r\"C:\\Users\\307\\Desktop\\aicup\\testing_image\"\n",
        "# dst_root = r\"C:\\Users\\307\\Desktop\\aicup\\test\\images\"\n",
        "\n",
        "# os.makedirs(dst_root, exist_ok=True)\n",
        "\n",
        "# # 收集所有圖片路徑\n",
        "# all_files = []\n",
        "# for patient_folder in os.listdir(src_root):\n",
        "#     patient_path = os.path.join(src_root, patient_folder)\n",
        "#     if os.path.isdir(patient_path) and patient_folder.startswith(\"patient\"):\n",
        "#         for fname in os.listdir(patient_path):\n",
        "#             if fname.endswith(\".png\"):\n",
        "#                 all_files.append(os.path.join(patient_path, fname))\n",
        "\n",
        "# # 按照檔名排序，方便重現結果\n",
        "# all_files.sort()\n",
        "\n",
        "# dst_file = os.path.join(dst_root, os.path.basename(f))\n",
        "# shutil.move(f, dst_file)\n",
        "\n",
        "# print(f\"完成移動！總共 {len(all_files)} 張，放到 images\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {},
      "outputs": [],
      "source": [
        "fold_results = [\n",
        "    {\"fold\": 1,\"best_weight\": r\"C:\\Users\\307\\Desktop\\aicup\\目前最好模型\\runs\\fold1\\exp\\weights\\best.pt\"},\n",
        "    {\"fold\": 2,\"best_weight\": r\"C:\\Users\\307\\Desktop\\aicup\\目前最好模型\\runs\\fold2\\exp\\weights\\best.pt\"},\n",
        "    {\"fold\": 3,\"best_weight\": r\"C:\\Users\\307\\Desktop\\aicup\\目前最好模型\\runs\\fold3\\exp\\weights\\best.pt\"},\n",
        "    {\"fold\": 4,\"best_weight\": r\"C:\\Users\\307\\Desktop\\aicup\\目前最好模型\\runs\\fold4\\exp\\weights\\best.pt\"},\n",
        "    {\"fold\": 5,\"best_weight\": r\"C:\\Users\\307\\Desktop\\aicup\\目前最好模型\\runs\\fold5\\exp\\weights\\best.pt\"}]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "測試資料夾：C:\\Users\\307\\Desktop\\aicup\\test\\images，找到 16620 張 PNG\n",
            "兩資料夾合計：16620 張\n",
            "Ensemble 推論 using: C:\\Users\\307\\Desktop\\aicup\\目前最好模型\\runs\\fold1\\exp\\weights\\best.pt\n",
            "Ensemble 推論 using: C:\\Users\\307\\Desktop\\aicup\\目前最好模型\\runs\\fold2\\exp\\weights\\best.pt\n",
            "Ensemble 推論 using: C:\\Users\\307\\Desktop\\aicup\\目前最好模型\\runs\\fold3\\exp\\weights\\best.pt\n",
            "Ensemble 推論 using: C:\\Users\\307\\Desktop\\aicup\\目前最好模型\\runs\\fold4\\exp\\weights\\best.pt\n",
            "Ensemble 推論 using: C:\\Users\\307\\Desktop\\aicup\\目前最好模型\\runs\\fold5\\exp\\weights\\best.pt\n",
            "示例： [('patient0065_0005', [])]\n"
          ]
        }
      ],
      "source": [
        "from pathlib import Path\n",
        "import os\n",
        "\n",
        "# 圖片輸入尺寸（例如：896×896）\n",
        "IMG_SIZE = 640\n",
        "\n",
        "# ---- 你要推論的兩個測試資料夾（Windows 路徑，用 r\"…\" 原始字串避免跳脫字元問題） ----\n",
        "TEST_DIRS = [\n",
        "    Path(r\"C:\\Users\\307\\Desktop\\aicup\\test\\images\"),\n",
        "]\n",
        "\n",
        "# ---- 選擇推論用的權重 ----\n",
        "# 方案 A：只用某一個 fold（資料劃分中的一個子集） 的 best.pt（最簡單，跑最少）\n",
        "USE_ENSEMBLE = True     # 若要改用多 fold 融合，設 True（下方有簡易投票版）\n",
        "\n",
        "# 如果只用單一 fold，指定 fold_id；若用 Ensemble，下方會自動抓全部 folds 的 best.pt\n",
        "SINGLE_FOLD_ID = 3\n",
        "\n",
        "# ---- 輔助：列出資料夾內所有 .png 影像（遞迴掃描） ----\n",
        "def list_pngs(folder: Path):\n",
        "    if not folder.exists():\n",
        "        raise FileNotFoundError(f\"找不到測試資料夾：{folder}\")\n",
        "    return sorted([p for p in folder.rglob(\"*.png\")])\n",
        "\n",
        "# ---- 輔助：把圖片路徑清單轉成 stem 清單（不含副檔名），之後提交檔會用到 ----\n",
        "def stems_from_paths(paths):\n",
        "    return [p.stem for p in paths]\n",
        "\n",
        "# ---- 推論函式（沿用你前面區塊的 run_inference，但這裡接受「路徑清單」而非 .txt） ----\n",
        "def run_inference_on_paths(weight_path: Path, img_paths: list, conf: float = 0.01):\n",
        "    \"\"\"\n",
        "    以 YOLO（You Only Look Once）權重對 img_paths（路徑清單）做推論\n",
        "    回傳 dict: img_stem -> [ (cls, score, x1,y1,x2,y2), ... ]\n",
        "    cls = 類別索引, score = 分數, x1,y1,x2,y2 = 框座標\n",
        "    \"\"\"\n",
        "    from ultralytics import YOLO\n",
        "    mdl = YOLO(str(weight_path))\n",
        "    results_dict = {}\n",
        "\n",
        "    for img in img_paths:\n",
        "        res = mdl.predict(source=str(img), imgsz=IMG_SIZE, conf=conf, verbose=False)[0]                  \n",
        "\n",
        "        dets = []\n",
        "        if res and res.boxes is not None and len(res.boxes) > 0:\n",
        "            for b in res.boxes:\n",
        "                xyxy = b.xyxy.cpu().numpy().astype(float)[0]\n",
        "                sc   = float(b.conf.cpu().numpy()[0])\n",
        "                cl   = int(b.cls.cpu().numpy()[0])\n",
        "                x1, y1, x2, y2 = [int(round(v)) for v in xyxy]\n",
        "                dets.append((cl, sc, x1, y1, x2, y2))\n",
        "        results_dict[Path(img).stem] = dets\n",
        "    return results_dict\n",
        "\n",
        "# ---- 如果開啟 Ensemble：簡單的「分數平均 + NMS（Non-Maximum Suppression）」融合（輕量示範版） ----\n",
        "# 說明：為了不引入外部 WBF（Weighted Box Fusion）套件，我們用簡化版融合：\n",
        "# - 對同一張圖，把多個模型的框全部收集\n",
        "# - 針對重疊( IOU >= 0.55 ) 的框做分組，分數取平均、座標取平均\n",
        "# - 這版不如 WBF 嚴謹，但易於整合、零額外依賴；要更強可改用 ensemble-boxes/WBF\n",
        "import math\n",
        "\n",
        "def iou_xyxy(a, b):\n",
        "    ax1, ay1, ax2, ay2 = a\n",
        "    bx1, by1, bx2, by2 = b\n",
        "    inter_x1 = max(ax1, bx1); inter_y1 = max(ay1, by1)\n",
        "    inter_x2 = min(ax2, bx2); inter_y2 = min(ay2, by2)\n",
        "    iw = max(0, inter_x2 - inter_x1); ih = max(0, inter_y2 - inter_y1)\n",
        "    inter = iw * ih\n",
        "    area_a = max(0, (ax2 - ax1)) * max(0, (ay2 - ay1))\n",
        "    area_b = max(0, (bx2 - bx1)) * max(0, (by2 - by1))\n",
        "    union = area_a + area_b - inter\n",
        "    return inter / union if union > 0 else 0.0\n",
        "\n",
        "def simple_ensemble_merge(list_of_preds_dicts, iou_thr=0.5):\n",
        "    \"\"\"\n",
        "    list_of_preds_dicts: [preds_dict_model1, preds_dict_model2, ...]\n",
        "      其中每個 preds_dict: img_stem -> [(cls, score, x1,y1,x2,y2), ...]\n",
        "    回傳同格式的 preds_merged\n",
        "    \"\"\"\n",
        "    merged = {}\n",
        "    # 先列出所有影像的 key（stem 的聯集）\n",
        "    all_stems = set()\n",
        "    for d in list_of_preds_dicts:\n",
        "        all_stems.update(d.keys())\n",
        "\n",
        "    for stem in all_stems:\n",
        "        # 收集此圖全部模型的框\n",
        "        all_boxes = []\n",
        "        for d in list_of_preds_dicts:\n",
        "            all_boxes.extend(d.get(stem, []))  # [(cls,score,x1,y1,x2,y2),...]\n",
        "\n",
        "        # 以簡易群聚方式融合\n",
        "        used = [False] * len(all_boxes)\n",
        "        fused = []\n",
        "        for i, bi in enumerate(all_boxes):\n",
        "            if used[i]:\n",
        "                continue\n",
        "            cls_i, sc_i, x1i, y1i, x2i, y2i = bi\n",
        "            group = [(cls_i, sc_i, x1i, y1i, x2i, y2i)]\n",
        "            used[i] = True\n",
        "            for j in range(i+1, len(all_boxes)):\n",
        "                if used[j]:\n",
        "                    continue\n",
        "                cls_j, sc_j, x1j, y1j, x2j, y2j = all_boxes[j]\n",
        "                # 只融合同類別且 IOU 達閾值的框\n",
        "                if cls_i == cls_j and iou_xyxy((x1i, y1i, x2i, y2i), (x1j, y1j, x2j, y2j)) >= iou_thr:\n",
        "                    group.append((cls_j, sc_j, x1j, y1j, x2j, y2j))\n",
        "                    used[j] = True\n",
        "            # 對 group 做平均（座標取平均、分數取平均）\n",
        "            if len(group) == 1:\n",
        "                fused.append(group[0])\n",
        "            else:\n",
        "                cls_g = group[0][0]\n",
        "                scores = [g[1] for g in group]\n",
        "                xs1 = [g[2] for g in group]; ys1 = [g[3] for g in group]\n",
        "                xs2 = [g[4] for g in group]; ys2 = [g[5] for g in group]\n",
        "                sc_m = sum(scores)/len(scores)\n",
        "                x1_m = int(round(sum(xs1)/len(xs1))); y1_m = int(round(sum(ys1)/len(ys1)))\n",
        "                x2_m = int(round(sum(xs2)/len(xs2))); y2_m = int(round(sum(ys2)/len(ys2)))\n",
        "                fused.append((cls_g, sc_m, x1_m, y1_m, x2_m, y2_m))\n",
        "        merged[stem] = fused\n",
        "    return merged\n",
        "\n",
        "# ---- 1) 掃描兩個資料夾的圖片 ----\n",
        "all_test_paths = []\n",
        "for td in TEST_DIRS:\n",
        "    imgs = list_pngs(td)\n",
        "    print(f\"測試資料夾：{td}，找到 {len(imgs)} 張 PNG\")\n",
        "    all_test_paths.extend(imgs)\n",
        "\n",
        "print(f\"兩資料夾合計：{len(all_test_paths)} 張\")\n",
        "\n",
        "# ---- 2) 跑推論（單一 fold 或 多 fold） ----\n",
        "if not USE_ENSEMBLE:\n",
        "    # 單 fold（簡單直接）\n",
        "    weight_path = Path(fold_results[SINGLE_FOLD_ID-1]['best_weight'])\n",
        "    assert weight_path.exists(), f\"找不到指定 fold 的權重：{weight_path}\"\n",
        "    preds_merged = run_inference_on_paths(weight_path, all_test_paths, conf=0.01)\n",
        "else:\n",
        "    # Ensemble（把每個 fold 的 best.pt 都跑一遍，最後融合）\n",
        "    preds_list = []\n",
        "    for fr in fold_results:\n",
        "        w = Path(fr[\"best_weight\"])\n",
        "        if not w.exists():\n",
        "            print(\"⚠️ 跳過不存在的權重：\", w)\n",
        "            continue\n",
        "        print(\"Ensemble 推論 using:\", w)\n",
        "        preds_list.append(run_inference_on_paths(w, all_test_paths, conf=0.01))\n",
        "    assert len(preds_list) > 0, \"沒有可用的權重可做 Ensemble。\"\n",
        "    preds_merged = simple_ensemble_merge(preds_list, iou_thr=0.5)\n",
        "\n",
        "# preds_merged 的格式：{img_stem: [(cls, score, x1,y1,x2,y2), ...], ...}\n",
        "print(\"示例：\", list(preds_merged.items())[:1])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# === 區塊 8（改版）：把兩資料夾的推論結果合併輸出成一個提交檔 ===\n",
        "# 官規：每行\n",
        "#   影像檔名(無 .png) 類別 分數 左上x 左上y 右下x 右下y\n",
        "# 注意：\n",
        "# - 沒有偵測的影像，不輸出任何行（不要寫空行）\n",
        "# - 這裡直接把 preds_merged（兩資料夾合併後）寫成一個 .txt\n",
        "\n",
        "def write_submission(preds: dict, out_path: Path):\n",
        "    lines = []\n",
        "    for stem, dets in preds.items(): \n",
        "        for (cls_id, score, x1, y1, x2, y2) in dets:\n",
        "            # 類別固定 0（主動脈瓣）；分數保留 4 位小數，座標為整數\n",
        "            lines.append(f\"{stem} {cls_id} {score:.4f} {x1} {y1} {x2} {y2}\")\n",
        "    out_path.write_text(\"\\n\".join(lines), encoding=\"utf-8\")\n",
        "    return len(lines)\n",
        "\n",
        "# 你可以自訂輸出檔名，例如：submission.txt\n",
        "out_file = SUBMIT_DIR / \"submission.txt\"\n",
        "n = write_submission(preds_merged, out_file)\n",
        "\n",
        "print(f\"✅ 已輸出提交檔：{out_file}\")\n",
        "print(f\"   行數（偵測結果數量）＝ {n}\")\n",
        "print(\"   規則提醒：只輸出有偵測的影像，無偵測不寫任何東西。\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import re\n",
        "from collections import defaultdict\n",
        "\n",
        "# === 基本設定 ===\n",
        "INPUT_FILE = r\"C:\\Users\\307\\Desktop\\aicup\\submissions\\submission.txt\"\n",
        "OUTPUT_FILE = r\"C:\\Users\\307\\Desktop\\aicup\\submissions\\submission_sorted.txt\"\n",
        "\n",
        "pattern = re.compile(r\"^(patient\\d+)_(\\d+)\\s+0\\s+([\\d.]+)\\s+(\\d+)\\s+(\\d+)\\s+(\\d+)\\s+(\\d+)\")\n",
        "patients = defaultdict(list)\n",
        "\n",
        "# === 讀檔與分組 ===\n",
        "with open(INPUT_FILE, \"r\", encoding=\"utf-8\") as f:\n",
        "    for line in f:\n",
        "        m = pattern.match(line.strip())\n",
        "        if not m:\n",
        "            continue\n",
        "        pid = m.group(1)\n",
        "        frame = int(m.group(2))\n",
        "        patients[pid].append((frame, line.strip()))\n",
        "\n",
        "# === 工具：找出連續群集 ===\n",
        "def find_clusters(frames):\n",
        "    frames = sorted(frames)\n",
        "    clusters = []\n",
        "    cluster = [frames[0]]\n",
        "    for i in range(1, len(frames)):\n",
        "        gap = frames[i] - frames[i-1]\n",
        "        if gap <= 5:  # 認為連續 frame 差距小於等於 2\n",
        "            cluster.append(frames[i])\n",
        "        else:\n",
        "            clusters.append(cluster)\n",
        "            cluster = [frames[i]]\n",
        "    clusters.append(cluster)\n",
        "    return clusters\n",
        "\n",
        "# === 主邏輯 ===\n",
        "keep_lines = []\n",
        "for pid, records in patients.items():\n",
        "    if len(records) <= 1:\n",
        "        keep_lines += [r[1] for r in records]\n",
        "        continue\n",
        "\n",
        "    frames = [r[0] for r in records]\n",
        "    clusters = find_clusters(frames)\n",
        "    # 找出最長 cluster (主連續區段)\n",
        "    main_cluster = max(clusters, key=len)\n",
        "\n",
        "    # 找出該 cluster 的範圍\n",
        "    start, end = min(main_cluster), max(main_cluster)\n",
        "    print(f\"{pid}: main range = {start}–{end} ({len(main_cluster)} frames)\")\n",
        "\n",
        "    # 保留範圍內的 line\n",
        "    for frame, line in records:\n",
        "        if start <= frame <= end:\n",
        "            keep_lines.append(line)\n",
        "\n",
        "# === 輸出 ===\n",
        "with open(OUTPUT_FILE, \"w\", encoding=\"utf-8\") as f:\n",
        "    for l in sorted(keep_lines):\n",
        "        f.write(l + \"\\n\")\n",
        "\n",
        "print(f\"✅ 自動偵測連續主區段完成，結果已儲存：{OUTPUT_FILE}\")\n",
        "print(f\"   原始行數：{sum(len(v) for v in patients.values())}，篩選後行數：{len(keep_lines)}\")\n",
        "\n",
        "# lines = open(INPUT_FILE, 'r').read().splitlines()\n",
        "# print(f\"總共有 {len(lines)} 筆預測結果\")\n",
        "# lines= sorted(lines)\n",
        "# with open(OUTPUT_FILE, 'w') as f:\n",
        "#     for line in lines:\n",
        "#         f.write(line + '\\n')\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.10"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
